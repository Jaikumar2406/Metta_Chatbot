import os
from dotenv import load_dotenv
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from pinecone import Pinecone
from langchain_groq import ChatGroq
from langchain_pinecone import PineconeVectorStore
from langchain_groq import ChatGroq 
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import MessagesPlaceholder
from langchain.chains import create_history_aware_retriever
from langchain.chains import create_retrieval_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory
from fastapi import FastAPI, HTTPException , Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import FileResponse
import json

app = FastAPI()

load_dotenv()

# app.mount("/assets", StaticFiles(directory="project/cliend/src"), name="src")

templates = Jinja2Templates(directory="templates")


class InputText(BaseModel):
    input: str

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


pine_cone = os.getenv("pine_cone")
groq = os.getenv("groq")
hugging_face = os.getenv("hugging_face")
host_bg = os.getenv("host_bg")

llm = ChatGroq(model='llama-3.3-70b-versatile' , api_key=groq , temperature=0.7)
embeddings = HuggingFaceBgeEmbeddings(model_name="BAAI/bge-base-en-v1.5" , model_kwargs = {"token":hugging_face})

pc = Pinecone(api_key = pine_cone)
index = pc.Index("metta" , host = host_bg)
vector = PineconeVectorStore(index=index , embedding=embeddings , text_key='page_content')
retriever = vector.as_retriever()

retriver_prompt = ("Given a chat history and the latest user question which might reference context in the chat history,"
    "formulate a standalone question which can be understood without the chat history."
    "Do NOT answer the question, just reformulate it if needed and otherwise return it as is.")
from langchain_core.prompts import ChatPromptTemplate
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
    ("system", retriver_prompt),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    ]
)

history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)

PRODUCT_BOT_TEMPLATE = """
    You are a MeTTa language code generator.

Instructions:
- The user will describe an NLP problem or task.
- You must generate a complete, runnable **MeTTa program** that solves the problem.
- Include all necessary rules, facts, and patterns.
- Do NOT explain in plain text or use any other language.
- Focus strictly on MeTTa code.

Example:
User: "I want to tokenize a sentence and remove stopwords."
Response (MeTTa code):

(tokenize_sentence "This is an example sentence." ?tokens)
(filter ?tokens (not (stopword ?token)) ?clean_tokens)
(print ?clean_tokens)  
    CONTEXT:
    {context}

    QUESTION: {input}

    YOUR ANSWER:

    """

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", PRODUCT_BOT_TEMPLATE),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

chat_history = []
store = {}

def get_session_history(session_id: str)-> BaseChatMessageHistory:
  if session_id not in store:
    store[session_id]= ChatMessageHistory()
  return store[session_id]

chain_with_memmory = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

def write_llm_text_to_json(text: str) -> str:
    """
    Convert LLM text into a JSON string (not saved to file).
    
    Args:
        text (str): The text generated by the LLM.
        
    Returns:
        str: JSON formatted string.
    """
    data = {"llm_output": text}
    return json.dumps(data, indent=4, ensure_ascii=False)



@app.get("/")
def home():
    return "backend is running"
# @app.get("/{full_path:path}")
# async def catch_all(full_path: str):
#     return FileResponse(os.path.join("frontend", "index.html"))




@app.post("/generate")
async def generate_text(data: InputText):
    user_input = data.input.strip()

    # Session end conditions
    if user_input.lower() in {"exit", "quit", "stop"}:
        return {"answer": "Session ended."}

    try:
        # Call your chain with memory
        result = chain_with_memmory.invoke(
            {"input": user_input},
            config={
                "configurable": {"session_id": "144"}
            },
        )
        # result = write_llm_text_to_json(result)



        return {"answer": result.get("answer", "I'm processing your request...")}


    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
